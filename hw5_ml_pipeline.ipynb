{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayeshahabib01/github-introfall25-ayeshahabib01/blob/main/hw5_ml_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLFbwFEJPPdu"
      },
      "source": [
        "# Homework 5 - Understanding Machine Learning Pipeline\n",
        "\n",
        "- Read the code below and understand the whole process of building a machine learning pipeline\n",
        "- Answer the 5 questions in the markdown cells\n",
        "- Store your answers and submit your ipynb file via Canvas\n",
        "- You CAN use any resources including internet and GenAI tools (remember you can use ChatGPT to help you understand the code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odBf9mvvPPdx"
      },
      "source": [
        "## A Machine Learning Pipeline for Titanic Dataset Survival Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKHKOcYPPPdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a63ab4a-71f9-46b3-da4f-3f8d8aabd988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1223568302.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "/tmp/ipython-input-1223568302.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
            "/tmp/ipython-input-1223568302.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['fare'].fillna(titanic['fare'].median(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-Fold Cross-Validation AccuracyScores: [0.74860335 0.79213483 0.86516854 0.79775281 0.8258427 ]\n",
            "Mean Accuracy Score: 0.8059004456719603\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Data cleaning\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "titanic['fare'].fillna(titanic['fare'].median(), inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "titanic['sex'] = LabelEncoder().fit_transform(titanic['sex'])\n",
        "titanic['embarked'] = LabelEncoder().fit_transform(titanic['embarked'].astype(str))\n",
        "\n",
        "# Define raw features\n",
        "X_raw = titanic[['pclass', 'sex', 'age', 'fare', 'sibsp', 'parch', 'embarked']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Feature engineering\n",
        "# Acutually some below features can be insightful, but some might be noise.\n",
        "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1 # family size = sibsp + parch + 1\n",
        "titanic['is_alone'] = (titanic['family_size'] == 1).astype(int) # is_alone = 1 if family size == 1, otherwise 0\n",
        "titanic['fare_bin'] = pd.qcut(titanic['fare'], 4, labels=[1, 2, 3, 4]).astype(int) # fare_bin = 1, 2, 3, 4; mapping fare into 4 bins\n",
        "titanic['age_fare_ratio'] = titanic['age'] / (titanic['fare'] + 1) # age_fare_ratio = age / (fare + 1)\n",
        "titanic['sibsp_parch_ratio'] = (titanic['sibsp'] + 1) / (titanic['parch'] + 1) # sibsp_parch_ratio = (sibsp + 1) / (parch + 1)\n",
        "titanic['age_class_interaction'] = titanic['age'] * titanic['pclass'] # age_class_interaction = age * pclass\n",
        "titanic['fare_per_family_member'] = titanic['fare'] / (titanic['family_size']) # fare_per_family_member = fare / (family_size)\n",
        "\n",
        "# Combine features\n",
        "X_engineered = titanic[['pclass', 'sex', 'age', 'fare', 'embarked', 'family_size', 'is_alone',\n",
        "                       'fare_bin', 'age_fare_ratio', 'sibsp_parch_ratio', 'age_class_interaction',\n",
        "                       'fare_per_family_member']]\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define pipeline with feature selection and scaling with Random Forest model\n",
        "pipeline = Pipeline([\n",
        "    ('feature_selection', SelectKBest(score_func=f_classif, k=6)), # select 6 features with the highest F-values versus full features\n",
        "    ('scaling', StandardScaler()),\n",
        "    ('model', model)\n",
        "])\n",
        "\n",
        "# Perform 5-fold cross-validation with the pipeline\n",
        "cv_scores = cross_val_score(pipeline, X_engineered, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Output cross-validation results\n",
        "print(\"5-Fold Cross-Validation AccuracyScores:\", cv_scores)\n",
        "print(\"Mean Accuracy Score:\", cv_scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc90YdY3PPdy"
      },
      "source": [
        "#### Question 1: please describe the process of building this machine learning pipeline - step by step.\n",
        "\n",
        "Your answer here: The machine learning pipeline is built with three main steps. First, SelectKBest is used to select the six features that have the strongest statistical relationship with the target variable, keeping only the most relevant information for the model. Next, StandardScaler scales these selected features so they have a mean of zero and a standard deviation of one, which helps the model perform better when features are on different scales. Finally, RandomForestClassifier is applied to train a model that can make predictions based on the processed features. The whole pipeline is then evaluated using 5-fold cross-validation, which splits the data into five parts, trains the model on four parts, tests it on the remaining part, and repeats this process to provide a reliable estimate of the model’s performance.\n",
        "\n",
        "#### Question 2: based on what you have learned in the lecture, please explain why we need to extract more features versus using raw features only.\n",
        "\n",
        "Your answer here: We need to extract more features because raw features don't always capture complex patterns or interactions in the data. Feature engineering lets us to create new variables that provide more information to the model, like combining or transforming existing features to highlight important relationships. For example, in the Titanic dataset, raw features like sibsp or parch alone may not fully reflect a passenger’s social context, but engineered features like family_size or is_alone capture these patterns more effectively.\n",
        "\n",
        "#### Question 3: based on what you have learned in the lecture, please explain why we need feature selection.\n",
        "\n",
        "Your answer here: We need feature selection because not all features contribute useful information to the model, and some may even introduce noise or irrelevant patterns. By selecting only the most important features, we can reduce the risk of overfitting, improve model accuracy, and make the model faster and easier to interpret. Feature selection also helps focus the model on the variables that matter most, which can simplify analysis and improve generalization to new data. In the pipeline, SelectKBest is used to automatically pick the six features that have the strongest statistical relationship with the target, keeping the most informative inputs for the model.\n",
        "\n",
        "#### Question 4: see printed results (accuracy scores across 5 folds versus averaged accuracy score), explain the benefits of using 5-fold CV versus one-time 80-20 split.\n",
        "\n",
        "Your answer here: Using 5-fold cross-validation has several benefits compared to a one-time 80-20 train-test split. In 5-fold CV, the data is split into five parts, and the model is trained on four parts and tested on the remaining part, repeating this process five times so that each part is used as a test set once. This approach provides multiple accuracy scores and a mean accuracy, which gives a more reliable estimate of model performance. On the other hand, a single 80-20 split only evaluates the model on one random subset, which can lead to biased or overly optimistic results depending on which samples fall into the training or test sets. 5-fold CV reduces this variance and makes sure that the model is evaluated on all data points. It also gives a better sense of how the model will generalize to unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39pfL0pAPPdz"
      },
      "source": [
        "## Compare the performance of different feature sets:\n",
        "- 1. raw features only;\n",
        "- 2. with engineered features;\n",
        "- 3. engineered features with feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDgEBR1kPPdz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "f06528e1-c485-4b63-a2fb-aef6c3a31d79"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Pipeline' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1419017983.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize a pipeline without feature selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m pipeline_no_selection = Pipeline([\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'scaling'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize a pipeline without feature selection\n",
        "pipeline_no_selection = Pipeline([\n",
        "    ('scaling', StandardScaler()),\n",
        "    ('model', model)\n",
        "])\n",
        "\n",
        "# 1. Raw features only\n",
        "cv_scores_raw = cross_val_score(pipeline_no_selection, X_raw, y, cv=5)\n",
        "\n",
        "# 2. Engineered features only (without feature selection)\n",
        "cv_scores_extraction = cross_val_score(pipeline_no_selection, X_engineered, y, cv=5)\n",
        "\n",
        "# 3. Engineered features with feature selection (global selection from previous block)\n",
        "cv_scores_full = cross_val_score(pipeline, X_engineered, y, cv=5)\n",
        "\n",
        "# Results summary\n",
        "results_df = pd.DataFrame({\n",
        "    'Experiment': [\n",
        "        'Raw Features Only',\n",
        "        'With Feature Extraction',\n",
        "        'With Feature Extraction & Feature Selection'\n",
        "    ],\n",
        "    'Mean Score': [\n",
        "        cv_scores_raw.mean(),\n",
        "        cv_scores_extraction.mean(),\n",
        "        cv_scores_full.mean()\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Display results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL2-w9USPPdz"
      },
      "source": [
        "#### Question 5: use formal language to explain printed results \"results_df\".\n",
        "\n",
        "Your answer here: The results in results_df compare the model’s performance across three different feature sets using 5-fold cross-validation.\n",
        "\n",
        "1) Raw Features Only: This baseline model uses only the original variables (pclass, sex, age, fare, sibsp, parch, embarked). Its mean accuracy reflects the predictive power of the raw data without any additional information.\n",
        "\n",
        "2) With Feature Extraction: When engineered features are added, the model’s mean accuracy improves. This indicates that the newly created features (e.g., family_size, is_alone, fare_bin) provide additional informative signals that help the model better distinguish between survivors and non-survivors.\n",
        "\n",
        "3) With Feature Extraction & Feature Selection: Applying feature selection on the engineered features slightly further improves or stabilizes the mean accuracy. This demonstrates that selecting the most relevant features reduces noise from less informative variables, enhancing model efficiency and predictive performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k21ii-T8PPd0"
      },
      "source": [
        "## Feature Importance Analysis\n",
        "- Note from lecturer:\n",
        "- Feature importance, in ML, measures how each feature contributes to the prediction accuracy of a machine learning model. It quantitatively reveals the influence of various factors on the model's predictive outcomes.\n",
        "- I found some final project groups proposed investigating the influence of certain factors on predictive outcomes but did not include a detailed methodological plan.\n",
        "- More specifically, descriptive analysis and visualizations are good for finding insights, but they are not enough to draw reliable quantitative conclusions.\n",
        "- For these groups, feature importance analysis might be a helpful approach to highlight the impact of different factors. I’ve attached the following code as a reference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9ZytShbPPd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85920c21-0fe0-46d8-8751-26a3486eed54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Feature  Importance Original or Engineered\n",
            "4   age_class_interaction    0.283923             Engineered\n",
            "1                     sex    0.260717               Original\n",
            "5  fare_per_family_member    0.191335             Engineered\n",
            "2                    fare    0.175876               Original\n",
            "0                  pclass    0.058137               Original\n",
            "3                fare_bin    0.030013             Engineered\n"
          ]
        }
      ],
      "source": [
        "# Fit pipeline on the entire dataset to select features\n",
        "pipeline.fit(X_engineered, y)\n",
        "\n",
        "# Get selected features\n",
        "selected_features_indices = pipeline.named_steps['feature_selection'].get_support(indices=True)\n",
        "selected_features = X_engineered.columns[selected_features_indices]\n",
        "\n",
        "# Scale the data using the fitted scaler\n",
        "X_selected_scaled = pipeline.named_steps['scaling'].transform(X_engineered.iloc[:, selected_features_indices])\n",
        "\n",
        "# Train model on the entire dataset with selected features for global feature importance\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_selected_scaled, y)\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "# Display selected features and their importance\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': selected_features,\n",
        "    'Importance': feature_importances,\n",
        "    'Original or Engineered': ['Engineered' if f in ['family_size', 'is_alone', 'fare_bin',\n",
        "                                                     'age_fare_ratio', 'sibsp_parch_ratio',\n",
        "                                                     'age_class_interaction', 'fare_per_family_member']\n",
        "                               else 'Original' for f in selected_features]\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display results\n",
        "print(feature_importance_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdnkKcgFPPd0"
      },
      "source": [
        "#### No question for feature importance :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OidYRoUqPPd1"
      },
      "source": [
        "Question 5: Considering these results, why might engineered features like age_class_interaction and fare_per_family_member outperform original features?\n",
        "\n",
        "\n",
        "Your answer here: Engineered features like age_class_interaction and fare_per_family_member can outperform original features because they capture more complex relationships and interactions in the data that raw features alone do not show. For example, age_class_interaction combines a passenger’s age and class, which reflects that survival likelihood may depend not just on age or class independently, but on how these factors interact. Similarly, fare_per_family_member normalizes the fare by family size, providing a more insightful measure of the financial resources available per individual rather than total fare alone. These engineered features provide the model with more informative signals that allow it to better distinguish patterns associated with survival, which increases predictive accuracy compared to using only original features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Why do you think sex is such an important predictor? Are there any ethical concerns with including this as a variable in a model? Explain your answers.\n",
        "\n",
        "\n",
        "Your answer here: The variable sex is an important predictor because, on the Titanic, men and women had very different survival rates due to the “women and children first” rule. This makes sex strongly linked to whether someone survived or not.\n",
        "However, using sex in a model can raise ethical concerns. In real-world applications, relying on sensitive attributes like gender can lead to biased or unfair decisions. While it is fine in this historical dataset for understanding past events, in modern situations we need to be careful to avoid discrimination and consider fairness when including such features."
      ],
      "metadata": {
        "id": "OQTpCrGnWMjX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yqOCv976W_GD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}